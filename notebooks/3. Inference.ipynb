{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n",
    "\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(2019)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "## get data\n",
    "# 1. vocabulary\n",
    "vocab, train, valid, test = data.get_data(os.path.join('../data/20ng'))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 2. tokens, counts for train, dev and test set\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11214"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7531"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '../results/etm_20ng_K_50_Htheta_800_Optim_adam_Clip_0.0_ThetaAct_relu_Lr_0.005_Bsz_1000_RhoSize_300_trainEmbeddings_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '20ng'\n",
    "num_topics = 50\n",
    "t_hidden_size = 800 # encoding dimension\n",
    "optimizer = 'adam'\n",
    "clip = 0\n",
    "theta_act = 'relu' # activation\n",
    "lr = 0.005 # learning rate\n",
    "wdecay=1.2e-6\n",
    "enc_drop = 0.0 # drop out rate on encoder\n",
    "batch_size = 1000\n",
    "rho_size = 300 # dimension of rho, the word embedding?\n",
    "emb_size = 300\n",
    "train_embeddings = 1\n",
    "\n",
    "embeddings = None\n",
    "model = ETM(num_topics,          # the all-important number of topics\n",
    "            vocab_size,          # vocab size is needed for input shape sizes, possibly redundant\n",
    "            t_hidden_size,       # t_hidden_size is the size of document encoding\n",
    "            rho_size,            # embedding size of word embedding \n",
    "            emb_size,            # embedding size of word embedding # redundant!\n",
    "            theta_act,           # activation function (string)\n",
    "            embeddings,          # prefit embeddings\n",
    "            train_embeddings,    # binary, for whether to train embeddings\n",
    "            enc_drop             # encoder dropout \n",
    "           ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_loss = 0\n",
    "acc_kl_theta_loss = 0\n",
    "cnt = 0\n",
    "\n",
    "num_docs_train = len(train_tokens)\n",
    "batch_size = 1000\n",
    "\n",
    "indices = torch.randperm(num_docs_train)\n",
    "indices = torch.split(indices, batch_size)\n",
    "idx, ind = 0, indices[0]\n",
    "\n",
    "data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "sums = data_batch.sum(1).unsqueeze(1)\n",
    "normalized_data_batch = data_batch / sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(405)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0: ['writes', 'article', 'org', 'jim', 'virginia', 'stanford', 'computer', 'distribution', 'usa']  \n",
    "Topic 1: ['mail', 'hp', 'line', 'mark', 'version', 'info', 'netcom', 'wrote', 'phone']  \n",
    "Topic 2: ['mit', 'ibm', 'mil', 'group', 'apr', 'navy', 'newsgroup', 'marc', 'list']  \n",
    "Topic 3: ['ftp', 'image', 'information', 'faq', 'pub', 'mail', 'data', 'software', 'graphics']  \n",
    "Topic 4: ['israel', 'jews', 'turkish', 'israeli', 'people', 'armenian', 'armenians', 'turkey', 'turks']  \n",
    "Topic 5: ['andrew', 'uiuc', 'cmu', 'colorado', 'university', 'michael', 'cso', 'writes', 'illinois']  \n",
    "Topic 6: ['drive', 'card', 'scsi', 'video', 'mac', 'system', 'bit', 'disk', 'hard']  \n",
    "Topic 7: ['good', 'writes', 'time', 'people', 'article', 'make', 'thing', 'things', 'back']  \n",
    "Topic 8: ['posting', 'host', 'university', 'nntp', 'article', 'writes', 'ca', 'cs', 'distribution']  \n",
    "Topic 9: ['writes', 'posting', 'article', 'university', 'host', 'nntp', 'ca', 'cs', 'distribution']  \n",
    "Topic 10: ['good', 'people', 'writes', 'time', 'make', 'thing', 'things', 'article', 'give']  \n",
    "Topic 11: ['good', 'time', 'writes', 'make', 'people', 'thing', 'back', 'bad', 'things']  \n",
    "Topic 12: ['writes', 'article', 'org', 'jim', 'usa', 'distribution', 'computer', 'stanford', 'opinions']  \n",
    "Topic 13: ['posting', 'host', 'nntp', 'ca', 'university', 'cc', 'distribution', 'article', 'cs']  \n",
    "Topic 14: ['windows', 'dos', 'files', 'access', 'ms', 'program', 'file', 'pc', 'run']  \n",
    "Topic 15: ['posting', 'host', 'nntp', 'university', 'ca', 'article', 'writes', 'cs', 'distribution']  \n",
    "Topic 16: ['uk', 'de', 'ac', 'au', 'uni', 'university', 'germany', 'tu', 'australia']  \n",
    "Topic 17: ['posting', 'host', 'university', 'nntp', 'ca', 'article', 'writes', 'cc', 'cs']  \n",
    "Topic 18: ['god', 'jesus', 'people', 'christian', 'christians', 'bible', 'church', 'christ', 'faith']  \n",
    "Topic 19: ['car', 'bike', 'dod', 'cars', 'engine', 'ride', 'road', 'front', 'speed']  \n",
    "Topic 20: ['writes', 'article', 'good', 'time', 'people', 'thing', 'make', 'back', 'heard']  \n",
    "Topic 21: ['max', 'ah', 'mr', 'air', 'ma', 'sp', 'cs', 'mi', 'tm']  \n",
    "Topic 22: ['space', 'nasa', 'gov', 'launch', 'henry', 'toronto', 'earth', 'jpl', 'satellite']  \n",
    "Topic 23: ['people', 'good', 'time', 'make', 'things', 'thing', 'work', 'real', 'lot']  \n",
    "Topic 24: ['writes', 'article', 'distribution', 'usa', 'university', 'org', 'computer', 'reply', 'david']  \n",
    "Topic 25: ['sale', 'price', 'offer', 'shipping', 'printer', 'sell', 'mail', 'condition', 'cd']  \n",
    "Topic 26: ['posting', 'host', 'nntp', 'ca', 'university', 'cc', 'article', 'distribution', 'cs']  \n",
    "Topic 27: ['people', 'told', 'home', 'time', 'started', 'left', 'building', 'back', 'day']  \n",
    "Topic 28: ['people', 'good', 'time', 'make', 'things', 'thing', 'years', 'work', 'put']  \n",
    "Topic 29: ['south', 'war', 'san', 'information', 'world', 'american', 'los', 'nuclear', 'southern']  \n",
    "Topic 30: ['good', 'time', 'make', 'back', 'work', 'problem', 'give', 'ago', 'long']  \n",
    "Topic 31: ['science', 'pitt', 'gordon', 'banks', 'cs', 'geb', 'food', 'article', 'disease']  \n",
    "Topic 32: ['writes', 'good', 'time', 'article', 'people', 'make', 'thing', 'back', 'give']  \n",
    "Topic 33: ['posting', 'university', 'host', 'nntp', 'writes', 'ca', 'article', 'cs', 'cc']  \n",
    "Topic 34: ['health', 'national', 'research', 'university', 'care', 'insurance', 'medical', 'children', 'rate']  \n",
    "Topic 35: ['good', 'time', 'make', 'problem', 'work', 'thing', 'back', 'level', 'find']  \n",
    "Topic 36: ['question', 'problem', 'read', 'post', 'point', 'time', 'answer', 'find', 'good']  \n",
    "Topic 37: ['power', 'water', 'wire', 'ground', 'current', 'circuit', 'high', 'cable', 'signal']  \n",
    "Topic 38: ['team', 'game', 'games', 'year', 'play', 'hockey', 'season', 'players', 'win']  \n",
    "Topic 39: ['netcom', 'hp', 'services', 'mail', 'newsreader', 'tin', 'mark', 'phone', 'version']  \n",
    "Topic 40: ['ohio', 'cwru', 'keith', 'cleveland', 'caltech', 'state', 'freenet', 'acs', 'sgi']  \n",
    "Topic 41: ['mail', 'software', 'information', 'line', 'find', 'version', 'net', 'number', 'info']  \n",
    "Topic 42: ['key', 'encryption', 'chip', 'clipper', 'government', 'keys', 'security', 'public', 'law']  \n",
    "Topic 43: ['writes', 'article', 'university', 'distribution', 'reply', 'usa', 'computer', 'org', 'cs']  \n",
    "Topic 44: ['people', 'law', 'article', 'rights', 'writes', 'sex', 'men', 'evidence', 'cramer']  \n",
    "Topic 45: ['book', 'books', 'question', 'theory', 'points', 'point', 'reference', 'find', 'time']  \n",
    "Topic 46: ['file', 'window', 'program', 'set', 'display', 'color', 'server', 'application', 'code']  \n",
    "Topic 47: ['good', 'time', 'make', 'thing', 'back', 'people', 'problem', 'work', 'things']  \n",
    "Topic 48: ['good', 'people', 'time', 'make', 'thing', 'things', 'back', 'give', 'work']  \n",
    "Topic 49: ['gun', 'people', 'mr', 'guns', 'government', 'president', 'clinton', 'weapons', 'control']  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict() method is as simple as :  \n",
    "```\n",
    "def predict_proba(self, normalized_bow):\n",
    "    return F.softmax( model.encode(normalized_bow) ).argmax()\n",
    "```\n",
    "Input: 2D pytorch array\n",
    "Output: probabilities\n",
    "  \n",
    "<font color=purple> Importantly, the choice of final activation layer has a lot of say in what kind of problem we want to solve.  \n",
    "    So for instance, softmax is great for single label multiclass problem, but sigmoid can be used for mult-label problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 50])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.alphas(model.rho.weight).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3072, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rho.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=300, out_features=3072, bias=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1220, -0.2369,  0.1000, -0.1858, -0.0396,  1.5132, -0.2654, -0.1114,\n",
       "        -0.1370, -0.1132, -0.0564, -0.1072, -0.1702, -0.1346, -0.1902, -0.1288,\n",
       "        -0.1792, -0.1313,  4.9024, -0.2944, -0.0665, -0.2247, -0.2041, -0.0585,\n",
       "        -0.1245, -0.1449, -0.1964, -0.0613, -0.0243, -0.1894, -0.1078,  0.5689,\n",
       "        -0.0595, -0.2002, -0.1898, -0.0687, -0.0373, -0.2176, -0.3027, -0.2731,\n",
       "        -0.2226, -0.2369, -0.1647, -0.0982, -0.0611, -0.1829, -0.1590, -0.0718,\n",
       "        -0.0703, -0.1583], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encode(normalized_data_batch[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4695, 0.4411, 0.5250, 0.4537, 0.4901, 0.8195, 0.4340, 0.4722, 0.4658,\n",
       "        0.4717, 0.4859, 0.4732, 0.4576, 0.4664, 0.4526, 0.4678, 0.4553, 0.4672,\n",
       "        0.9926, 0.4269, 0.4834, 0.4441, 0.4492, 0.4854, 0.4689, 0.4639, 0.4511,\n",
       "        0.4847, 0.4939, 0.4528, 0.4731, 0.6385, 0.4851, 0.4501, 0.4527, 0.4828,\n",
       "        0.4907, 0.4458, 0.4249, 0.4321, 0.4446, 0.4411, 0.4589, 0.4755, 0.4847,\n",
       "        0.4544, 0.4603, 0.4820, 0.4824, 0.4605], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.sigmoid( model.encode(normalized_data_batch[0])[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinitrinh/anaconda3/envs/gr2/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0049, 0.0043, 0.0061, 0.0046, 0.0053, 0.0250, 0.0042, 0.0049, 0.0048,\n",
       "        0.0049, 0.0052, 0.0049, 0.0046, 0.0048, 0.0045, 0.0048, 0.0046, 0.0048,\n",
       "        0.7403, 0.0041, 0.0051, 0.0044, 0.0045, 0.0052, 0.0049, 0.0048, 0.0045,\n",
       "        0.0052, 0.0054, 0.0046, 0.0049, 0.0097, 0.0052, 0.0045, 0.0045, 0.0051,\n",
       "        0.0053, 0.0044, 0.0041, 0.0042, 0.0044, 0.0043, 0.0047, 0.0050, 0.0052,\n",
       "        0.0046, 0.0047, 0.0051, 0.0051, 0.0047], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax( model.encode(normalized_data_batch[0])[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['modern',\n",
       " 'law',\n",
       " 'cs',\n",
       " 'link',\n",
       " 'longer',\n",
       " 'sin',\n",
       " 'days',\n",
       " 'uga',\n",
       " 'athens',\n",
       " 'kind',\n",
       " 'paul',\n",
       " 'day',\n",
       " 'food',\n",
       " 'christians',\n",
       " 'eat',\n",
       " 'cc',\n",
       " 'sunday',\n",
       " 'recognized',\n",
       " 'totally',\n",
       " 'question',\n",
       " 'keeping',\n",
       " 'personal',\n",
       " 'jesus',\n",
       " 'athena',\n",
       " 'authority',\n",
       " 'telling',\n",
       " 'holy',\n",
       " 'judge',\n",
       " 'requirements',\n",
       " 'irrelevant',\n",
       " 'people',\n",
       " 'writes',\n",
       " 'fourth',\n",
       " 'acts',\n",
       " 'command',\n",
       " 'living',\n",
       " 'jewish',\n",
       " 'position',\n",
       " 'article',\n",
       " 'man',\n",
       " 'regard',\n",
       " 'georgia',\n",
       " 'mailer',\n",
       " 'romans',\n",
       " 'relevant',\n",
       " 'list',\n",
       " 'jr',\n",
       " 'problem',\n",
       " 'university']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"document words\" )\n",
    "[vocab[idx] for idx in train_tokens[ind[0]][0] ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # writing stops to a list in a py file\n",
    "# with open('../scripts/stops.txt', 'r') as f:\n",
    "#     stops = f.read()\n",
    "# with open('stops.py', 'w') as f:\n",
    "#     f.write('stops = [\"' + stops.replace('\\n', '\",\\n\"')[:-1] + ']')\n",
    "# from stops import stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
