{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### Model Internal pipeline\n",
    "This notebook examines the internal workings and pipeline of unlabelled ETM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n",
    "\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(2019)\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### Input data\n",
    "The input data contains three important objects: `vocab`, `tokens`, `counts`. \n",
    "`vocab` is the actual string words, while `token` are indices that lead to the respective words and `counts`, as the name suggests, counts the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ii', 'plate', 'duke', 'greatly', 'holds']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  94,  100,  233,  327,  357,  504,  530,  597,  662,  720,  805,\n",
       "        859,  889,  897,  898,  987,  996, 1024, 1098, 1177, 1178, 1532,\n",
       "       1642, 1658, 1706, 1728, 1732, 1808, 1822, 1857, 1858, 1890, 1895,\n",
       "       1957, 2013, 2059, 2091, 2118, 2174, 2236, 2386, 2478, 2522, 2539,\n",
       "       2566, 2569, 2662, 2673, 2790, 2812, 2887, 2934, 3018, 3048, 3071],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1,\n",
       "       1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get data\n",
    "# 1. vocabulary\n",
    "vocab, train, valid, test = data.get_data(os.path.join('../data/20ng'))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 2. tokens, counts for train, dev and test set\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "display( vocab[:5] )\n",
    "display( train_tokens[:2][0][0] )\n",
    "display( train_counts[:2][0][0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### Settings and checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '20ng'\n",
    "num_topics = 50\n",
    "t_hidden_size = 800 # encoding dimension\n",
    "optimizer = 'adam'\n",
    "clip = 0\n",
    "theta_act = 'relu' # activation\n",
    "lr = 0.005 # learning rate\n",
    "wdecay=1.2e-6\n",
    "enc_drop = 0.0 # drop out rate on encoder\n",
    "batch_size = 1000\n",
    "rho_size = 300 # dimension of rho, the word embedding?\n",
    "emb_size = 300\n",
    "train_embeddings = 1\n",
    "    \n",
    "ckpt = os.path.join(\"train_ckpt\", \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        dataset, num_topics, t_hidden_size, optimizer, clip, theta_act, \n",
    "            lr, batch_size, rho_size, train_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "Importantly, there are no pre-trained embeddings here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### Init the model  \n",
    "ETM is a pytorch module with a whole bunch of parameters. \n",
    "Some questions would be:  \n",
    "    <font color=\"red\">  \n",
    "1. Whats the difference between `emsize` and `rho_size`? One should be the topic embedding size, while the other should be the word embedding, but they should be of the same dimension and even related to each other.   \n",
    "2. What is `t_hidden_size`?\n",
    "        \n",
    "        \n",
    "```\n",
    "class ETM(nn.Module):\n",
    "def __init__(self, num_topics, vocab_size, t_hidden_size, rho_size, emsize, \n",
    "                theta_act, embeddings=None, train_embeddings=True, enc_drop=0.5):\n",
    "    super(ETM, self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\" \n",
    "    Define hyperparameters\n",
    "    \"\"\"\n",
    "    self.num_topics = num_topics\n",
    "    self.vocab_size = vocab_size\n",
    "        \n",
    "    self.t_hidden_size = t_hidden_size # ?\n",
    "    \n",
    "    self.rho_size = rho_size\n",
    "    self.emsize = emsize\n",
    "        \n",
    "    self.enc_drop = enc_drop\n",
    "    self.t_drop = nn.Dropout(enc_drop)\n",
    "\n",
    "    self.theta_act = self.get_activation(theta_act)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    define the word embedding matrix \\rho\n",
    "    \"\"\"\n",
    "    if train_embeddings:\n",
    "        self.rho = nn.Linear(rho_size, vocab_size, bias=False)\n",
    "    else:\n",
    "        num_embeddings, emsize = embeddings.size()\n",
    "        rho = nn.Embedding(num_embeddings, emsize)\n",
    "        self.rho = embeddings.clone().float().to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    define the matrix containing the topic embeddings\n",
    "    \"\"\"\n",
    "    self.alphas = nn.Linear(rho_size, num_topics, bias=False)#nn.Parameter(torch.randn(rho_size, num_topics))\n",
    "\n",
    "    ## define variational distribution for \\theta_{1:D} via amortizartion\n",
    "    self.q_theta = nn.Sequential(\n",
    "            nn.Linear(vocab_size, t_hidden_size), \n",
    "            self.theta_act,\n",
    "            nn.Linear(t_hidden_size, t_hidden_size),\n",
    "            self.theta_act,\n",
    "        )\n",
    "    self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "    self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = None\n",
    "model = ETM(num_topics,        # the all-important number of topics\n",
    "            vocab_size,        # vocab size is needed for input shape sizes, possibly redundant\n",
    "            t_hidden_size,     # t_hidden_size is the size of document encoding\n",
    "            rho_size,          # embedding size of word embedding \n",
    "            emb_size,          # embedding size of word embedding # redundant!\n",
    "            theta_act,         # activation function (string)\n",
    "            embeddings,        # prefit embeddings\n",
    "            train_embeddings,  # binary, for whether to train embeddings\n",
    "            enc_drop           # encoder dropout \n",
    "           ).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "Set to training mode:   \n",
    "https://pytorch.org/docs/stable/nn.html  \n",
    "Essentially, this 'activates' layers specifically for training, namely `dropout`, `BatchNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "Setting the train indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_loss = 0\n",
    "acc_kl_theta_loss = 0\n",
    "cnt = 0\n",
    "\n",
    "num_docs_train = len(train_tokens)\n",
    "batch_size = 1000\n",
    "\n",
    "indices = torch.randperm(num_docs_train)\n",
    "indices = torch.split(indices, batch_size)\n",
    "idx, ind = 0, indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "As typical of pytorch training scripts, set to zero gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wdecay)\n",
    "optimizer.zero_grad()\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### retrieving batch train data\n",
    "`data_batch` will be of shape (batch size, vocab size).   \n",
    "`sums` is the sum of words in each doc which may be used to normalized the bag of words, `bow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 3072])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[61.],\n",
       "        [27.],\n",
       "        [66.],\n",
       "        [34.]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "sums = data_batch.sum(1).unsqueeze(1)\n",
    "normalized_data_batch = data_batch / sums\n",
    "\n",
    "display(data_batch.shape)\n",
    "display(sums[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "### Forward pass within model\n",
    "There are two inputs: `data_batch` and `normalized_data_batch`. \n",
    "`recon_loss, kld_theta = model(data_batch, normalized_data_batch)`\n",
    "\n",
    "The forward function is as such:  \n",
    "```\n",
    "    def forward(self, bows, normalized_bows, theta=None, aggregate=True):\n",
    "        ## get \\theta\n",
    "        if theta is None:\n",
    "            theta, kld_theta = self.get_theta(normalized_bows)\n",
    "        else:\n",
    "            kld_theta = None\n",
    "\n",
    "        ## get \\beta\n",
    "        beta = self.get_beta()\n",
    "\n",
    "        ## get prediction loss\n",
    "        preds = self.decode(theta, beta)\n",
    "        recon_loss = -(preds * bows).sum(1)\n",
    "        if aggregate:\n",
    "            recon_loss = recon_loss.mean()\n",
    "        return recon_loss, kld_theta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>   \n",
    "## Forward pass #1: Retreiving topic proportions, theta\n",
    "The first part of the forward pass is to get the topic proportions, `theta`, which really is $\\delta$ in the paper.  \n",
    "This tells us how much of each document belongs to each topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>   \n",
    "`normalized_data_batch` is first encoded.  \n",
    "This will give us a `theta` which really is $\\delta$ in the paper. It refers to the topic proportions, that is, how much of each document belongs to each topic. Thus, `theta` is of shape (batch size, no. of topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply variational params, the neural network\n",
    "# this gives us the document encoding, q, of shape (batchsize, t_hidden_size)\n",
    "q_theta = model.q_theta(normalized_data_batch)\n",
    "\n",
    "# apply dropout \n",
    "if model.enc_drop > 0:\n",
    "    q_theta = model.t_drop(q_theta)\n",
    "\n",
    "# the output layer of the inference network\n",
    "# \"The inference network ingest the doocument wd\n",
    "# and outputs a mean and a variance of delta_d (theta)\"\n",
    "mu_theta = model.mu_q_theta(q_theta)\n",
    "logsigma_theta = model.logsigma_q_theta(q_theta)\n",
    "\n",
    "# Now to input mu and sigma \n",
    "# to sample theta.\n",
    "# theta is a logistic normal model\n",
    "# I dont understand how to derive \"eps.mul_(std).add_(mu_theta)\" !!!!!!!!!!!!!!!!!!!\n",
    "std = torch.exp(0.5 * logsigma_theta) \n",
    "eps = torch.randn_like(std)\n",
    "z = eps.mul_(std).add_(mu_theta)\n",
    "theta = torch.nn.functional.softmax(z, dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 50])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>   \n",
    "The loss function of forward pass #1 is in `kld_theta` or `kl_theta`.  \n",
    "The authors write: _\"[This] term encourages [the topic proportions, $\\delta_{d}$] to be close to the prior, $p(\\delta_{d})$_.\"  \n",
    "This may prove useful for seeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is this the kullback leibler? \n",
    "kld_theta = kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>   \n",
    "## Forward pass #2: Retrieve beta  \n",
    "    \n",
    "\"`beta` denotes a traditional topic; ie, a distribution over all the words.\"  \n",
    "Thus, it makes sense that beta follows the shape (no. of topics, size of vocab)\n",
    "    </br>  \n",
    "    \n",
    "If `model.alpha` is the topic representation and `model.rho` is the word representation, then their dot products would be the agreement between topic and words.  \n",
    "\n",
    "</br>  \n",
    "  \n",
    "The following softmax function over this agreement values intuitively gives a probability distribution over the following words, that is:   \n",
    "$$softmax(\\alpha \\cdot \\rho) = \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: Linear(in_features=300, out_features=50, bias=False) - (embsize x topics)\n",
      "logit: torch.Size([3072, 300]) - (vocab x embsize)\n",
      "beta shape: torch.Size([50, 3072])\n"
     ]
    }
   ],
   "source": [
    "print( f'alpha: {model.alphas} - (embsize x topics)' )\n",
    "print( f'logit: {model.rho.weight.shape} - (vocab x embsize)' )\n",
    "\n",
    "logit = model.alphas(model.rho.weight)\n",
    "beta = torch.nn.functional.softmax(logit, dim=0).transpose(1, 0)\n",
    "\n",
    "print( f\"beta shape: {beta.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>   \n",
    "## Forward pass #3: Reconstruct bag of words and calculate loss\n",
    "How much does our model agree with the data? This is observed in `recon_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________\n",
      "actual bag of words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________\n",
      "predicted bag of words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-8.0245, -8.0295, -8.0288,  ..., -8.0267, -8.0191, -8.0231],\n",
       "        [-8.0304, -8.0275, -8.0292,  ..., -8.0236, -8.0212, -8.0251],\n",
       "        [-8.0257, -8.0284, -8.0244,  ..., -8.0295, -8.0160, -8.0211],\n",
       "        ...,\n",
       "        [-8.0320, -8.0251, -8.0266,  ..., -8.0343, -8.0200, -8.0196],\n",
       "        [-8.0291, -8.0227, -8.0251,  ..., -8.0234, -8.0173, -8.0263],\n",
       "        [-8.0300, -8.0369, -8.0218,  ..., -8.0206, -8.0241, -8.0191]],\n",
       "       device='cuda:0', grad_fn=<LogBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "__________\n",
      "reconstruction loss shape before aggregating: 1000\n",
      "reconstruction loss: 668.0684204101562\n"
     ]
    }
   ],
   "source": [
    "# decode\n",
    "res = torch.mm(theta, beta)\n",
    "preds = torch.log(res+1e-6) # +1e-6 to prevent log(0) errro\n",
    "recon_loss = -(preds * data_batch).sum(1)\n",
    "\n",
    "print('\\n' + '_'*10)\n",
    "print(\"actual bag of words\")\n",
    "display( data_batch )\n",
    "\n",
    "print('\\n' + '_'*10)\n",
    "print(\"predicted bag of words\")\n",
    "display( preds )\n",
    "\n",
    "print('\\n' + '_'*10)\n",
    "print( f\"reconstruction loss shape before aggregating: {recon_loss.shape[0]}\" )\n",
    "\n",
    "recon_loss = recon_loss.mean()\n",
    "print( f\"reconstruction loss: {recon_loss}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=purple>  \n",
    "## Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = recon_loss + kld_theta\n",
    "total_loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
