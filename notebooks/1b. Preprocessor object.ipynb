{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor object\n",
    "Can we develop a preprocessor object to carry all that information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw inputs\n",
    "The raw input is a list of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\"]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train') # from sklearn\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "train_data.data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired  outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ii', 'plate', 'duke', 'greatly', 'holds']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  94,  100,  233,  327,  357,  504,  530,  597,  662,  720,  805,\n",
       "        859,  889,  897,  898,  987,  996, 1024, 1098, 1177, 1178, 1532,\n",
       "       1642, 1658, 1706, 1728, 1732, 1808, 1822, 1857, 1858, 1890, 1895,\n",
       "       1957, 2013, 2059, 2091, 2118, 2174, 2236, 2386, 2478, 2522, 2539,\n",
       "       2566, 2569, 2662, 2673, 2790, 2812, 2887, 2934, 3018, 3048, 3071],\n",
       "      dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       2, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1,\n",
       "       1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('..')\n",
    "import data\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## get data\n",
    "# 1. vocabulary\n",
    "vocab, train, valid, test = data.get_data(os.path.join('../data/20ng'))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 2. tokens, counts for train, dev and test set\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)\n",
    "\n",
    "display( vocab[:5] )\n",
    "display( train_tokens[:2][0][0] )\n",
    "display( train_counts[:2][0][0] )\n",
    "\n",
    "acc_loss = 0\n",
    "acc_kl_theta_loss = 0\n",
    "cnt = 0\n",
    "\n",
    "num_docs_train = len(train_tokens)\n",
    "batch_size = 1000\n",
    "\n",
    "indices = torch.randperm(num_docs_train)\n",
    "indices = torch.split(indices, batch_size)\n",
    "idx, ind = 0, indices[0]\n",
    "\n",
    "data_batch = data.get_batch(train_tokens, train_counts, ind, vocab_size, device)\n",
    "sums = data_batch.sum(1).unsqueeze(1)\n",
    "normalized_data_batch = data_batch / sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import numbers\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "from stops import stops\n",
    "\n",
    "class Preprocessor(CountVectorizer):\n",
    "    \n",
    "    def __init__(self, min_df=0.005, max_df=0.7):\n",
    "        super().__init__(min_df=min_df, max_df=max_df, stop_words=stops)\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    CLEANING METHODS\n",
    "    \"\"\"\n",
    "    def clean_doc(self, doc):\n",
    "        \"\"\"\n",
    "        Removes words with puncutations / numbers and one letter words\n",
    "        \"\"\"\n",
    "        list_of_tokens = re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', doc)\n",
    "\n",
    "        def contains_punctuation(w):\n",
    "            return any(char in string.punctuation for char in w)\n",
    "        def contains_numeric(w):\n",
    "            return any(char.isdigit() for char in w)\n",
    "\n",
    "        # filter out words with punctuation, eg \"where's\"\n",
    "        list_of_tokens = [token.lower() for token in list_of_tokens if not contains_punctuation(token)]\n",
    "        # filter out words with numbers, eg \"rac3\"\n",
    "        list_of_tokens = [token for token in list_of_tokens if not contains_numeric(token)]\n",
    "        # remove one letter words\n",
    "        list_of_tokens = [token for token in list_of_tokens if len(token)>1]\n",
    "\n",
    "        return ' '.join(list_of_tokens)\n",
    "    \n",
    "    def clean_corpus(self, corpus):\n",
    "        \"\"\"\n",
    "        Cleans iterable of string docs\n",
    "        \"\"\"\n",
    "        return [clean_doc(doc) for doc in corpus]\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    FIT AND PREPROCESS\n",
    "    \"\"\"\n",
    "    def fit_transform(self, raw_documents, y=None):\n",
    "        \"\"\"Learn the vocabulary dictionary and return term-document matrix.\n",
    "        This is equivalent to fit followed by transform, but more efficiently\n",
    "        implemented.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : array, [n_samples, n_features]\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        # We intentionally don't call the transform method to make\n",
    "        # fit_transform overridable without unwanted side effects in\n",
    "        # TfidfVectorizer.\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        self._validate_params()\n",
    "        self._validate_vocabulary()\n",
    "        max_df = self.max_df\n",
    "        min_df = self.min_df\n",
    "        max_features = self.max_features\n",
    "\n",
    "        vocabulary, X = self._count_vocab(self.clean_corpus(raw_documents),\n",
    "                                          self.fixed_vocabulary_)\n",
    "\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "\n",
    "        if not self.fixed_vocabulary_:\n",
    "            X = self._sort_features(X, vocabulary)\n",
    "\n",
    "            n_doc = X.shape[0]\n",
    "            max_doc_count = (max_df\n",
    "                             if isinstance(max_df, numbers.Integral)\n",
    "                             else max_df * n_doc)\n",
    "            min_doc_count = (min_df\n",
    "                             if isinstance(min_df, numbers.Integral)\n",
    "                             else min_df * n_doc)\n",
    "            if max_doc_count < min_doc_count:\n",
    "                raise ValueError(\n",
    "                    \"max_df corresponds to < documents than min_df\")\n",
    "            X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
    "                                                       max_doc_count,\n",
    "                                                       min_doc_count,\n",
    "                                                       max_features)\n",
    "\n",
    "            self.vocabulary_ = vocabulary\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def _transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to document-term matrix.\n",
    "        Extract token counts out of raw text documents using the vocabulary\n",
    "        fitted with fit or the one provided to the constructor.\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw_documents : iterable\n",
    "            An iterable which yields either str, unicode or file objects.\n",
    "        Returns\n",
    "        -------\n",
    "        X : sparse matrix, [n_samples, n_features]\n",
    "            Document-term matrix.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "        self._check_vocabulary()\n",
    "\n",
    "        # use the same matrix-building strategy as fit_transform\n",
    "        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
    "        if self.binary:\n",
    "            X.data.fill(1)\n",
    "        return X\n",
    "    \n",
    "    def transform(self, raw_documents, y=None):\n",
    "        return self._transform(self.clean_corpus(raw_documents))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron', 'ab', 'ability', 'abortion', 'absolute']\n",
      "[[3 1 0 ... 0 0 0]]\n",
      "['zealand', 'zip', 'zone', 'zoo', 'zuma']\n",
      "[[0 0 0 ... 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "# # how to get vocab from the stuff\n",
    "# # https://stackoverflow.com/questions/28894756/countvectorizer-does-not-print-vocabulary/44320484\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "# test_set = (\"The sun in the sky is bright.\", \n",
    "#     \"We can see the shining sun, the bright sun.\")\n",
    "\n",
    "# vectorizer = CountVectorizer(stop_words='english')\n",
    "# document_term_matrix = vectorizer.fit_transform(train_set)\n",
    "# print( vectorizer.get_feature_names() )\n",
    "\n",
    "# print( vectorizer.transform(['blue blue blue']).toarray() )\n",
    "\n",
    "asd = Preprocessor()\n",
    "cvz = asd.fit_transform(train_data.data)\n",
    "\n",
    "print( asd.get_feature_names()[:5] )\n",
    "print( asd.transform(['aaron aaron aaron ab']).toarray() )\n",
    "\n",
    "print( asd.get_feature_names()[-5:] )\n",
    "print( asd.transform(['zuma zuma zuma']).toarray() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting this, the right words came out, i.e. its the same words as from the authors output.  \n",
    "```\n",
    "array(['addition', 'body', 'brought', 'called', 'car', 'college', 'day',\n",
    "       'door', 'doors', 'early', 'engine', 'front', 'history', 'host',\n",
    "       'il', 'info', 'late', 'looked', 'made', 'mail', 'maryland',\n",
    "       'model', 'nntp', 'park', 'posting', 'production', 'rest',\n",
    "       'separate', 'small', 'specs', 'sports', 'thing', 'umd',\n",
    "       'university', 'wondering', 'years'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['addition', 'body', 'brought', 'called', 'car', 'college', 'day',\n",
       "       'door', 'doors', 'early', 'engine', 'front', 'history', 'host',\n",
       "       'il', 'info', 'late', 'looked', 'made', 'mail', 'maryland',\n",
       "       'model', 'nntp', 'park', 'posting', 'production', 'rest',\n",
       "       'separate', 'small', 'specs', 'sports', 'thing', 'umd',\n",
       "       'university', 'wondering', 'years'], dtype='<U15')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = cvz[0].toarray()[0]\n",
    "np.array(asd.get_feature_names())[(row > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " ':',\n",
       " 'lerxst',\n",
       " '@',\n",
       " 'wam',\n",
       " '.',\n",
       " 'umd',\n",
       " '.',\n",
       " 'edu',\n",
       " '(',\n",
       " \"where's\",\n",
       " 'my',\n",
       " 'thing',\n",
       " ')',\n",
       " 'Subject',\n",
       " ':',\n",
       " 'WHAT',\n",
       " 'car',\n",
       " 'is',\n",
       " 'this',\n",
       " '!',\n",
       " '?',\n",
       " 'Nntp',\n",
       " 'Posting',\n",
       " 'Host',\n",
       " ':',\n",
       " 'rac3',\n",
       " '.',\n",
       " 'wam',\n",
       " '.',\n",
       " 'umd',\n",
       " '.',\n",
       " 'edu',\n",
       " 'Organization',\n",
       " ':',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Maryland',\n",
       " ',',\n",
       " 'College',\n",
       " 'Park',\n",
       " 'Lines',\n",
       " ':',\n",
       " '15',\n",
       " 'I',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'if',\n",
       " 'anyone',\n",
       " 'out',\n",
       " 'there',\n",
       " 'could',\n",
       " 'enlighten',\n",
       " 'me',\n",
       " 'on',\n",
       " 'this',\n",
       " 'car',\n",
       " 'I',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'other',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'a',\n",
       " '2',\n",
       " 'door',\n",
       " 'sports',\n",
       " 'car',\n",
       " ',',\n",
       " 'looked',\n",
       " 'to',\n",
       " 'be',\n",
       " 'from',\n",
       " 'the',\n",
       " 'late',\n",
       " '60s',\n",
       " '/',\n",
       " 'early',\n",
       " '70s',\n",
       " '.',\n",
       " 'It',\n",
       " 'was',\n",
       " 'called',\n",
       " 'a',\n",
       " 'Bricklin',\n",
       " '.',\n",
       " 'The',\n",
       " 'doors',\n",
       " 'were',\n",
       " 'really',\n",
       " 'small',\n",
       " '.',\n",
       " 'In',\n",
       " 'addition',\n",
       " ',',\n",
       " 'the',\n",
       " 'front',\n",
       " 'bumper',\n",
       " 'was',\n",
       " 'separate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'body',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'know',\n",
       " '.',\n",
       " 'If',\n",
       " 'anyone',\n",
       " 'can',\n",
       " 'tellme',\n",
       " 'a',\n",
       " 'model',\n",
       " 'name',\n",
       " ',',\n",
       " 'engine',\n",
       " 'specs',\n",
       " ',',\n",
       " 'years',\n",
       " 'of',\n",
       " 'production',\n",
       " ',',\n",
       " 'where',\n",
       " 'this',\n",
       " 'car',\n",
       " 'is',\n",
       " 'made',\n",
       " ',',\n",
       " 'history',\n",
       " ',',\n",
       " 'or',\n",
       " 'whatever',\n",
       " 'info',\n",
       " 'you',\n",
       " 'have',\n",
       " 'on',\n",
       " 'this',\n",
       " 'funky',\n",
       " 'looking',\n",
       " 'car',\n",
       " ',',\n",
       " 'please',\n",
       " 'e',\n",
       " 'mail',\n",
       " '.',\n",
       " 'Thanks',\n",
       " ',',\n",
       " 'IL',\n",
       " 'brought',\n",
       " 'to',\n",
       " 'you',\n",
       " 'by',\n",
       " 'your',\n",
       " 'neighborhood',\n",
       " 'Lerxst']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1. Read data\n",
    "\"\"\"\n",
    "\n",
    "print('reading data...')\n",
    "train_data = fetch_20newsgroups(subset='train') # from sklearn\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Filter out words with numerics and punctuations\n",
    "\"\"\"\n",
    "\n",
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', train_data.data[doc]) for doc in range(len(train_data.data))]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', test_data.data[doc]) for doc in range(len(test_data.data))]\n",
    "\n",
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)\n",
    "    \n",
    "# put the train and test set together\n",
    "init_docs = init_docs_tr + init_docs_ts\n",
    "# filter out words with punctuation, eg \"where's\"\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "# filter out words with numbers, eg \"rac3\"\n",
    "# init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "# # remove one letter words\n",
    "# init_docs = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "# # Join the words back together into whole string documents\n",
    "# init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]\n",
    "\n",
    "init_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
