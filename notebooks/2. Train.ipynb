{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np \n",
    "import os \n",
    "import math \n",
    "import random \n",
    "import sys\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from etm import ETM\n",
    "from utils import nearest_neighbors, get_topic_coherence, get_topic_diversity\n",
    "\n",
    "np.random.seed(2019)\n",
    "torch.manual_seed(2019)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get data__, namely the tokens and counts. This is parsed by the `data` class that the authors wrote.  \n",
    "The key function, `_fetch`, retrieves the data and parses into a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data\n",
    "# 1. vocabulary\n",
    "vocab, train, valid, test = data.get_data(os.path.join('../data/20ng'))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 2. tokens, counts for train, dev and test set\n",
    "train_tokens = train['tokens']\n",
    "train_counts = train['counts']\n",
    "num_docs_train = len(train_tokens)\n",
    "valid_tokens = valid['tokens']\n",
    "valid_counts = valid['counts']\n",
    "num_docs_valid = len(valid_tokens)\n",
    "test_tokens = test['tokens']\n",
    "test_counts = test['counts']\n",
    "num_docs_test = len(test_tokens)\n",
    "test_1_tokens = test['tokens_1']\n",
    "test_1_counts = test['counts_1']\n",
    "num_docs_test_1 = len(test_1_tokens)\n",
    "test_2_tokens = test['tokens_2']\n",
    "test_2_counts = test['counts_2']\n",
    "num_docs_test_2 = len(test_2_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either train the embeddings from scratch or load pre-trained ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to a directory `ckpt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## define checkpoint\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)\n",
    "\n",
    "if args.mode == 'eval':\n",
    "    ckpt = args.load_from\n",
    "else:\n",
    "    ckpt = os.path.join(args.save_path, \n",
    "        'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(\n",
    "        args.dataset, args.num_topics, args.t_hidden_size, args.optimizer, args.clip, args.theta_act, \n",
    "            args.lr, args.batch_size, args.rho_size, args.train_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Init the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model and optimizer\n",
    "model = ETM(args.num_topics, vocab_size, args.t_hidden_size, args.rho_size, args.emb_size, \n",
    "                args.theta_act, embeddings, args.train_embeddings, args.enc_drop).to(device)\n",
    "\n",
    "print('model: {}'.format(model))\n",
    "\n",
    "if args.optimizer == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adagrad':\n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'rmsprop':\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n",
    "elif args.optimizer == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n",
    "else:\n",
    "    print('Defaulting to vanilla SGD')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The training script, ie epochs and evaluate__  \n",
    "Notably, there is a `visualize` and an 'annealing' part of the script; where the learning rate is divided by a constant when closer to a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model on data \n",
    "best_epoch = 0\n",
    "best_val_ppl = 1e9\n",
    "all_val_ppls = []\n",
    "print('\\n')\n",
    "print('Visualizing model quality before training...')\n",
    "visualize(model)\n",
    "print('\\n')\n",
    "for epoch in range(1, args.epochs):\n",
    "    train(epoch)\n",
    "    val_ppl = evaluate(model, 'val')\n",
    "    if val_ppl < best_val_ppl:\n",
    "        with open(ckpt, 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_epoch = epoch\n",
    "        best_val_ppl = val_ppl\n",
    "    else:\n",
    "        ## check whether to anneal lr\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        if args.anneal_lr and (len(all_val_ppls) > args.nonmono and val_ppl > min(all_val_ppls[:-args.nonmono]) and lr > 1e-5):\n",
    "            optimizer.param_groups[0]['lr'] /= args.lr_factor\n",
    "    if epoch % args.visualize_every == 0:\n",
    "        visualize(model)\n",
    "    all_val_ppls.append(val_ppl)\n",
    "with open(ckpt, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model = model.to(device)\n",
    "val_ppl = evaluate(model, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    acc_loss = 0\n",
    "    acc_kl_theta_loss = 0\n",
    "    cnt = 0\n",
    "    indices = torch.randperm(args.num_docs_train)\n",
    "    indices = torch.split(indices, args.batch_size)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        data_batch = data.get_batch(train_tokens, train_counts, ind, args.vocab_size, device)\n",
    "        sums = data_batch.sum(1).unsqueeze(1)\n",
    "        if args.bow_norm:\n",
    "            normalized_data_batch = data_batch / sums\n",
    "        else:\n",
    "            normalized_data_batch = data_batch\n",
    "        recon_loss, kld_theta = model(data_batch, normalized_data_batch)\n",
    "        total_loss = recon_loss + kld_theta\n",
    "        total_loss.backward()\n",
    "\n",
    "        if args.clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        acc_loss += torch.sum(recon_loss).item()\n",
    "        acc_kl_theta_loss += torch.sum(kld_theta).item()\n",
    "        cnt += 1\n",
    "\n",
    "        if idx % args.log_interval == 0 and idx > 0:\n",
    "            cur_loss = round(acc_loss / cnt, 2) \n",
    "            cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "            cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "\n",
    "            print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "                epoch, idx, len(indices), optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    \n",
    "    cur_loss = round(acc_loss / cnt, 2) \n",
    "    cur_kl_theta = round(acc_kl_theta_loss / cnt, 2) \n",
    "    cur_real_loss = round(cur_loss + cur_kl_theta, 2)\n",
    "    print('*'*100)\n",
    "    print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {}'.format(\n",
    "            epoch, optimizer.param_groups[0]['lr'], cur_kl_theta, cur_loss, cur_real_loss))\n",
    "    print('*'*100)\n",
    "\n",
    "def visualize(m, show_emb=True):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    queries = ['andrew', 'computer', 'sports', 'religion', 'man', 'love', \n",
    "                'intelligence', 'money', 'politics', 'health', 'people', 'family']\n",
    "\n",
    "    ## visualize topics using monte carlo\n",
    "    with torch.no_grad():\n",
    "        print('#'*100)\n",
    "        print('Visualize topics...')\n",
    "        topics_words = []\n",
    "        gammas = m.get_beta()\n",
    "        for k in range(args.num_topics):\n",
    "            gamma = gammas[k]\n",
    "            top_words = list(gamma.cpu().numpy().argsort()[-args.num_words+1:][::-1])\n",
    "            topic_words = [vocab[a] for a in top_words]\n",
    "            topics_words.append(' '.join(topic_words))\n",
    "            print('Topic {}: {}'.format(k, topic_words))\n",
    "\n",
    "        if show_emb:\n",
    "            ## visualize word embeddings by using V to get nearest neighbors\n",
    "            print('#'*100)\n",
    "            print('Visualize word embeddings by using output embedding matrix')\n",
    "            try:\n",
    "                embeddings = m.rho.weight  # Vocab_size x E\n",
    "            except:\n",
    "                embeddings = m.rho         # Vocab_size x E\n",
    "            neighbors = []\n",
    "            for word in queries:\n",
    "                print('word: {} .. neighbors: {}'.format(\n",
    "                    word, nearest_neighbors(word, embeddings, vocab)))\n",
    "            print('#'*100)\n",
    "\n",
    "def evaluate(m, source, tc=False, td=False):\n",
    "    \"\"\"Compute perplexity on document completion.\n",
    "    \"\"\"\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        if source == 'val':\n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_valid)), args.eval_batch_size)\n",
    "            tokens = valid_tokens\n",
    "            counts = valid_counts\n",
    "        else: \n",
    "            indices = torch.split(torch.tensor(range(args.num_docs_test)), args.eval_batch_size)\n",
    "            tokens = test_tokens\n",
    "            counts = test_counts\n",
    "\n",
    "        ## get \\beta here\n",
    "        beta = m.get_beta()\n",
    "\n",
    "        ### do dc and tc here\n",
    "        acc_loss = 0\n",
    "        cnt = 0\n",
    "        indices_1 = torch.split(torch.tensor(range(args.num_docs_test_1)), args.eval_batch_size)\n",
    "        for idx, ind in enumerate(indices_1):\n",
    "            ## get theta from first half of docs\n",
    "            data_batch_1 = data.get_batch(test_1_tokens, test_1_counts, ind, args.vocab_size, device)\n",
    "            sums_1 = data_batch_1.sum(1).unsqueeze(1)\n",
    "            if args.bow_norm:\n",
    "                normalized_data_batch_1 = data_batch_1 / sums_1\n",
    "            else:\n",
    "                normalized_data_batch_1 = data_batch_1\n",
    "            theta, _ = m.get_theta(normalized_data_batch_1)\n",
    "\n",
    "            ## get prediction loss using second half\n",
    "            data_batch_2 = data.get_batch(test_2_tokens, test_2_counts, ind, args.vocab_size, device)\n",
    "            sums_2 = data_batch_2.sum(1).unsqueeze(1)\n",
    "            res = torch.mm(theta, beta)\n",
    "            preds = torch.log(res)\n",
    "            recon_loss = -(preds * data_batch_2).sum(1)\n",
    "            \n",
    "            loss = recon_loss / sums_2.squeeze()\n",
    "            loss = loss.mean().item()\n",
    "            acc_loss += loss\n",
    "            cnt += 1\n",
    "        cur_loss = acc_loss / cnt\n",
    "        ppl_dc = round(math.exp(cur_loss), 1)\n",
    "        print('*'*100)\n",
    "        print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))\n",
    "        print('*'*100)\n",
    "        if tc or td:\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            if tc:\n",
    "                print('Computing topic coherence...')\n",
    "                get_topic_coherence(beta, train_tokens, vocab)\n",
    "            if td:\n",
    "                print('Computing topic diversity...')\n",
    "                get_topic_diversity(beta, 25)\n",
    "        return ppl_dc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
