{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the preprocessing steps\n",
    "The authors preprocess the 20 news group.   \n",
    "Importantly, this is a bag of words model, so attention should be paid to watch __stop words__ and how the corpus is __parsed into its tokens in their native documents.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Reading and parsing the data into its tokens\n",
    "The authors download the data.   \n",
    "One document from the raw data looks like:\n",
    "```\n",
    "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\"\n",
    "```\n",
    "\n",
    "The authors then parse the documents into words using `re`. This is not meant to tokenize it but to filter out the words that they do not need, namely words with puntuations or numerics. \n",
    "\n",
    "Notably, even though both the train and test datasets are downloaded, they are merged to a total of 18846 documents anyway, of which 11260 docs are in the training set and 7532 in the test set and 100 as a validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'from lerxst wam umd edu my thing subject what car is this nntp posting host wam umd edu organization university of maryland college park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Read data\n",
    "\"\"\"\n",
    "\n",
    "print('reading data...')\n",
    "# train_data = fetch_20newsgroups(subset='train') # from sklearn\n",
    "# test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "2. Filter out words with numerics and punctuations\n",
    "\"\"\"\n",
    "\n",
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', train_data.data[doc]) for doc in range(len(train_data.data))]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', test_data.data[doc]) for doc in range(len(test_data.data))]\n",
    "\n",
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "def contains_numeric(w):\n",
    "    return any(char.isdigit() for char in w)\n",
    "    \n",
    "# put the train and test set together\n",
    "init_docs = init_docs_tr + init_docs_ts\n",
    "# filter out words with punctuation, eg \"where's\"\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "# filter out words with numbers, eg \"rac3\"\n",
    "init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "# remove one letter words\n",
    "init_docs = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "# Join the words back together into whole string documents\n",
    "init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]\n",
    "\n",
    "init_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Count vectorize the corpus and get the vocab\n",
    "\n",
    "`type(cvz) == scipy.sparse.csr.csr_matrix`\n",
    "Notably, `sign()` is used. Meaning, the values of this sparse matrix is binary, indicatoing whether the word is present, but ignoring how many. <font color=red> (why should the authors do this?) </font>\n",
    "```\n",
    ">>> sparse.csr.csr_matrix([[1,2,3,0,-1]]).sign().toarray()\n",
    "array([[ 1,  1,  1,  0, -1]], dtype=int64)\n",
    "```\n",
    "\n",
    "`cvz` is of shape (18846, 19148), which is (no of docs, no of vocab).  \n",
    "Needless to say, this is after the 70% document count filtering.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document frequency & stop words \n",
    "The authors write: _\"We preprocess the corpus by filtering stop words, words with document frequency above 70% and tokenizing.\"_  \n",
    "`min_df` and `max_df` filter out words at the left-tail and right-tail of document frequency.   \n",
    "Stop words are also listed and removed. \n",
    "```\n",
    "['a',\n",
    " 'able',\n",
    " 'about',\n",
    " .\n",
    " .\n",
    " .\n",
    " 'yourself', \n",
    " 'yourselves',\n",
    " 'z',\n",
    " 'zero',\n",
    " '']\n",
    " ````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`  \n",
    "`max_df` : float in range [0.0, 1.0] or int, default=1.0\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold (corpus-specific\n",
    "    stop words).\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "`min_df` : float in range [0.0, 1.0] or int, default=1\n",
    "    When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold. This value is also\n",
    "    called cut-off in the literature.\n",
    "    If float, the parameter represents a proportion of documents, integer\n",
    "    absolute counts.\n",
    "    This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n",
      "building the vocabulary...\n"
     ]
    }
   ],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.7\n",
    "min_df = 0.005  # choose desired value for min_df\n",
    "\n",
    "# Read stopwords\n",
    "with open('../scripts/stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "\n",
    "\n",
    "# Create count vectorizer\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=stops)\n",
    "cvz = cvectorizer.fit_transform(init_docs).sign()\n",
    "\n",
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0)\n",
    "v_size = sum_counts.shape[1]\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from lerxst wam umd edu my thing subject what car is this nntp posting host wam umd edu organization university of maryland college park lines was wondering if anyone out there could enlighten me on this car saw the other day it was door sports car looked to be from the late early it was called bricklin the doors were really small in addition the front bumper was separate from the rest of the body this is all know if anyone can tellme model name engine specs years of production where this car is made history or whatever info you have on this funky looking car please mail thanks il brought to you by your neighborhood lerxst'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cvz[0].toarray()>0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['addition', 'body', 'brought', 'called', 'car', 'college', 'day',\n",
       "       'door', 'doors', 'early', 'engine', 'front', 'history', 'host',\n",
       "       'il', 'info', 'late', 'looked', 'made', 'mail', 'maryland',\n",
       "       'model', 'nntp', 'park', 'posting', 'production', 'rest',\n",
       "       'separate', 'small', 'specs', 'sports', 'thing', 'umd',\n",
       "       'university', 'wondering', 'years'], dtype='<U14')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(cvectorizer.get_feature_names())[(cvz[0].toarray()>0)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point of this code snippet is to get `word2id` and `id2word` in an order from infrequent to common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  initial vocabulary size: 3275\n"
     ]
    }
   ],
   "source": [
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) \n",
    "                for w in cvectorizer.vocabulary_])\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) \n",
    "                for w in cvectorizer.vocabulary_])\n",
    "# del cvectorizer\n",
    "print('  initial vocabulary size: {}'.format(v_size))\n",
    "\n",
    "# Sort elements in vocabulary from infrequent to common words\n",
    "idx_sort = np.argsort(sum_counts_np)\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n",
    "\n",
    "# Create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([118, 197, 349, ..., 146, 135, 133])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 275, 2702,  920, ..., 2198,  169, 3250])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Train Val Test split\n",
    "Pretty simply, the authors carefully split the corpus into train test split. One additional thing they did was restrict the vocabulary to the train set vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n",
      "  vocabulary after removing words not in train: 3275\n",
      "  number of documents (train): 11214 [this should be equal to 11214]\n",
      "  number of documents (test): 7532 [this should be equal to 7532]\n",
      "  number of documents (valid): 100 [this should be equal to 100]\n",
      "removing empty documents...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SPLIT DOCS INTO TRAIN-TEST-VALID\n",
    "\"\"\"\n",
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs_tr = len(init_docs_tr)\n",
    "trSize = num_docs_tr-100\n",
    "tsSize = len(init_docs_ts)\n",
    "vaSize = 100\n",
    "idx_permute = np.random.permutation(num_docs_tr).astype(int)\n",
    "\n",
    "# Remove words not in train_data\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in init_docs[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n",
    "\n",
    "# Split in train/test/valid\n",
    "docs_tr = [[word2id[w] for w in init_docs[idx_permute[idx_d]].split() if w in word2id] for idx_d in range(trSize)]\n",
    "docs_va = [[word2id[w] for w in init_docs[idx_permute[idx_d+trSize]].split() if w in word2id] for idx_d in range(vaSize)]\n",
    "docs_ts = [[word2id[w] for w in init_docs[idx_d+num_docs_tr].split() if w in word2id] for idx_d in range(tsSize)]\n",
    "\n",
    "print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize))\n",
    "print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize))\n",
    "print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "REMOVE EMPTY OR SINGLE WORD DOCUMENTS\n",
    "\"\"\"\n",
    "# Remove empty documents\n",
    "print('removing empty documents...')\n",
    "\n",
    "def remove_empty(in_docs):\n",
    "    return [doc for doc in in_docs if doc!=[]]\n",
    "\n",
    "docs_tr = remove_empty(docs_tr)\n",
    "docs_ts = remove_empty(docs_ts)\n",
    "docs_va = remove_empty(docs_va)\n",
    "\n",
    "# Remove test documents with length=1\n",
    "docs_ts = [doc for doc in docs_ts if len(doc)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index of the first training set is 2551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs_tr[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors split the test set into 2 halves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting test documents in 2 halves...\n"
     ]
    }
   ],
   "source": [
    "# Split test set in 2 halves\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Get bag of word matrices\n",
    "In order to get the bag of word matrices, the authors go through quite a number of steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating lists of words...\n",
      "  len(words_tr):  934666\n",
      "  len(words_ts):  606887\n",
      "  len(words_ts_h1):  301564\n",
      "  len(words_ts_h2):  305323\n",
      "  len(words_va):  12953\n"
     ]
    }
   ],
   "source": [
    "# Getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr): ', len(words_tr))\n",
    "print('  len(words_ts): ', len(words_ts))\n",
    "print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('  len(words_va): ', len(words_va))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting doc indices...\n",
      "  len(np.unique(doc_indices_tr)): 11214 [this should be 11214]\n",
      "  len(np.unique(doc_indices_ts)): 7531 [this should be 7531]\n",
      "  len(np.unique(doc_indices_ts_h1)): 7531 [this should be 7531]\n",
      "  len(np.unique(doc_indices_ts_h2)): 7531 [this should be 7531]\n",
      "  len(np.unique(doc_indices_va)): 100 [this should be 100]\n"
     ]
    }
   ],
   "source": [
    "# Get doc indices\n",
    "print('getting doc indices...')\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))\n",
    "\n",
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "# # Remove unused variables\n",
    "# del docs_tr\n",
    "# del docs_ts\n",
    "# del docs_ts_h1\n",
    "# del docs_ts_h2\n",
    "# del docs_va\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating bow representation...\n"
     ]
    }
   ],
   "source": [
    "# Create bow representation\n",
    "print('creating bow representation...')\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "# del words_tr\n",
    "# del words_ts\n",
    "# del words_ts_h1\n",
    "# del words_ts_h2\n",
    "# del words_va\n",
    "# del doc_indices_tr\n",
    "# del doc_indices_ts\n",
    "# del doc_indices_ts_h1\n",
    "# del doc_indices_ts_h2\n",
    "# del doc_indices_va"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bow_tr` is a sparse matrix of shape (number of documents x vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11214x3275 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 614491 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7187f72fd6ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minit_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2551\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/gr2/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m                                                        max_features)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gr2/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_limit_features\u001b[0;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[1;32m   1110\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "cvectorizer.fit_transform([init_docs[2551]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 4, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_tr.getrow(0).toarray()[(bow_tr.getrow(0).toarray()>1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19097"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coo_matrix:  \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Save the bag of words\n",
    "The authors take each `bow` and saves them separately as tokens and counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the vocabulary to a file\n",
    "path_save = './min_df_' + str(min_df) + '/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.system('mkdir -p ' + path_save)\n",
    "\n",
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "del vocab\n",
    "\n",
    "# Split bow intro token/value pairs\n",
    "print('splitting bow intro token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts\n",
    "\n",
    "print('Data ready !!')\n",
    "print('*************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
